{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"imgs/image.png\" width=500>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torchtext\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```INPUT (0, 2, 11, 24, 123, 1)``` - token idx\n",
    "\n",
    "```OUTPUT ([0.533, ... , 0.123], [0.627, ... , 0.156], ... , [0.724, ... , 0.976])``` - token embedding \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tranformer_config():\n",
    "    def __init__(self) -> None:\n",
    "        self.emb_dim = 512\n",
    "        self.ffn_dim = 2048\n",
    "        self.num_heads = 8\n",
    "        self.num_layers = 6\n",
    "        self.dropout = 0.1\n",
    "        self.max_len = 256\n",
    "        self.batch_size = 1\n",
    "        self.lr = 1e-4\n",
    "        self.epochs = 10\n",
    "        self.vocab_size = 50257\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding_with_pe(nn.Module):\n",
    "    def __init__(self, vocab_size, sequence_length, embedding_dim):\n",
    "        \"\"\"\n",
    "        input: tensor of tokens (batch_size, sequence_length, embedding_dim)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sequence_range = torch.arange(sequence_length).unsqueeze(0)\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.position_encoding = nn.Embedding(sequence_length, embedding_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pe = self.position_encoding(self.sequence_range)\n",
    "        embedding = self.embeddings(x)\n",
    "        return embedding + pe\n",
    "        \n",
    "\n",
    "# embed = Embedding_with_pe(vocab_size=10, sequence_lenght=5, embedding_dim=3)\n",
    "# embed(torch.tensor([[1, 2, 0, 0, 0]]))\n",
    "# embed.sequence_range\n",
    "# embed(torch.tensor([[1,2,3,4]])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tembed = Embedding_with_pe(vocab_size=20, embedding_dim=10, sequence_lenght=4)\n",
    "# embed(torch.randint(0, 20, [2, 4])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MHA + Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention_AddNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.MHA = nn.MultiheadAttention(embed_dim=embedding_dim, num_heads=n_heads, dropout=dropout, batch_first=True)\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        attn_output, _ = self.MHA(Q, K, V, attn_mask=mask)\n",
    "        attn_output = self.dropout(attn_output)\n",
    "        output = self.layer_norm(Q + attn_output)\n",
    "        return output\n",
    "\n",
    "# seq_len = 100\n",
    "# emb_dim = 160\n",
    "# n_heads = 8 \n",
    "# batch = 32\n",
    "\n",
    "# mask = torch.triu(torch.ones(seq_len, seq_len), diagonal=1).bool()\n",
    "# # print(mask)\n",
    "\n",
    "# mha = MultiHeadAttention_AddNorm(emb_dim, n_heads, dropout=0)\n",
    "# output, attention = mha(torch.randn(batch, seq_len, emb_dim), mask=None)\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed Forward + Add & Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FF_AddNorm(nn.Module):\n",
    "    def __init__(self, embedding_dim, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, ffn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(ffn_dim, embedding_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        self.layer_norm = nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        ff_output = self.ff(x)\n",
    "        output = self.layer_norm(x + ff_output)\n",
    "        return output\n",
    "\n",
    "\n",
    "# seq_len = 100\n",
    "# emb_dim = 160\n",
    "# n_heads = 8 \n",
    "# batch = 32\n",
    "\n",
    "# ff = FF_AddNorm(emb_dim, 512, 0.1)\n",
    "# output = ff(torch.randn(batch, seq_len, emb_dim))\n",
    "\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention_AddNorm(embedding_dim, n_heads, dropout)\n",
    "        self.ffn = FF_AddNorm(embedding_dim, ffn_dim, dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.attention(x, x, x, mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_blocks, embedding_dim, n_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([EncoderLayer(embedding_dim, n_heads, ffn_dim, dropout) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "    \n",
    "# n_blocks = 6\n",
    "# embedding_dim = 512 \n",
    "# n_heads = 8\n",
    "# ffn_dim = 2048\n",
    "\n",
    "# encoder = Encoder(n_blocks, embedding_dim, n_heads, ffn_dim)\n",
    "# output = encoder(torch.randn(32, 100, 512))\n",
    "\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, embedding_dim, n_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention_AddNorm(embedding_dim, n_heads, dropout)\n",
    "        self.cross_attention = MultiHeadAttention_AddNorm(embedding_dim, n_heads, dropout)\n",
    "        self.ffn = FF_AddNorm(embedding_dim, ffn_dim, dropout)\n",
    "        \n",
    "    def forward(self, x, context, target_mask=None, padding_mask=None):\n",
    "        x = self.self_attention(x, x, x, target_mask)\n",
    "        x = self.cross_attention(x, context, context, padding_mask)\n",
    "        x = self.ffn(x)\n",
    "        return x\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_blocks, embedding_dim, n_heads, ffn_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([DecoderLayer(embedding_dim, n_heads, ffn_dim, dropout) for _ in range(n_blocks)])\n",
    "\n",
    "    def forward(self, x, context, target_mask=None, padding_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, context, target_mask, padding_mask)\n",
    "        return x\n",
    "    \n",
    "# decoder = Decoder(n_blocks, embedding_dim, n_heads, ffn_dim)\n",
    "# output = decoder(torch.randn(32, 100, 512), torch.randn(32, 100, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embdedding original ---------------------------------------------------------------------------------------------------- \n",
      " tensor([[[ 0.1353, -0.5520, -0.2578,  ..., -3.0753,  0.1462, -2.6390],\n",
      "         [-1.6448, -2.4551, -2.1762,  ...,  0.5636, -1.2615, -2.9863],\n",
      "         [ 0.6010,  0.5996, -0.0621,  ..., -0.0647,  0.5029, -0.0619],\n",
      "         ...,\n",
      "         [ 0.4387, -2.4390,  0.3713,  ...,  0.3974, -1.2088,  1.1083],\n",
      "         [-0.0412, -0.4828, -1.4971,  ..., -2.5934,  1.4022, -0.4571],\n",
      "         [ 2.1110,  0.1720,  0.6893,  ..., -1.2799,  0.1511, -0.3131]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "embdedding target ---------------------------------------------------------------------------------------------------- \n",
      " tensor([[[ 1.1583, -1.4360, -1.0492,  ..., -2.1828,  1.5707, -0.3493],\n",
      "         [-1.1209, -2.8678, -2.5269,  ...,  0.6984, -1.8902, -2.4611],\n",
      "         [ 1.2762,  0.5925,  0.2262,  ...,  2.8743, -0.5552,  2.1794],\n",
      "         ...,\n",
      "         [ 0.1383, -1.0510,  0.2185,  ..., -0.7502,  0.4134,  2.9147],\n",
      "         [-2.1690, -0.5605, -1.8032,  ..., -0.0304,  0.4631,  0.1152],\n",
      "         [ 1.9646, -1.5517,  0.7020,  ..., -1.0213,  0.9719,  2.4347]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "encoder_out ---------------------------------------------------------------------------------------------------- \n",
      " tensor([[[ 0.1871, -0.1281, -0.8363,  ..., -2.7083,  0.6872, -1.7605],\n",
      "         [-0.9844, -1.2330, -1.2161,  ...,  0.3876, -0.7160, -1.5062],\n",
      "         [ 1.2386,  0.9217,  0.1710,  ...,  0.4054,  0.5359,  0.4494],\n",
      "         ...,\n",
      "         [ 0.6411, -1.6870, -0.5006,  ...,  1.2186, -1.0080,  0.3803],\n",
      "         [ 1.0859,  0.6240, -0.1357,  ..., -1.9713,  2.0073,  0.1399],\n",
      "         [ 2.0683, -0.1522,  0.6247,  ...,  0.0592,  0.7978,  0.1551]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "decoder_out ---------------------------------------------------------------------------------------------------- \n",
      " tensor([[[ 0.1338, -1.1178,  0.1076,  ..., -1.7321,  1.7596,  0.0581],\n",
      "         [-0.5252, -1.4367, -1.0934,  ...,  0.2396, -1.6984,  0.4516],\n",
      "         [-0.0298,  0.7563, -0.6684,  ...,  1.5203, -0.3614,  1.1784],\n",
      "         ...,\n",
      "         [-0.3160, -1.3358, -0.3158,  ..., -0.7822,  0.6543,  1.8146],\n",
      "         [-1.7909, -0.8458, -1.5014,  ..., -0.9702,  0.2241,  0.5748],\n",
      "         [ 0.5927, -1.1213, -0.1475,  ..., -1.1140,  0.3077,  0.9847]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n",
      "output ---------------------------------------------------------------------------------------------------- \n",
      " tensor([[[-0.5267,  1.0796, -0.1615,  ...,  1.3761,  0.0058, -0.8249],\n",
      "         [-0.2519, -0.0144,  0.1606,  ...,  0.3186,  0.2321,  1.0275],\n",
      "         [ 0.0739,  0.1607,  0.3077,  ..., -0.1822,  0.4173, -0.1218],\n",
      "         ...,\n",
      "         [ 0.0409,  0.1633,  0.3477,  ..., -0.3976,  0.6189,  0.4467],\n",
      "         [-0.0927, -0.5373, -0.3651,  ...,  0.3124, -0.4914,  0.2436],\n",
      "         [-0.4781,  0.1264, -0.4461,  ...,  0.0823,  0.6559, -0.1587]]],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.embedding = Embedding_with_pe(config.vocab_size, config.max_len, config.emb_dim)\n",
    "        self.encoder = Encoder(config.num_encoder_layers, config.emb_dim, config.num_heads, config.ffn_dim, config.dropout)\n",
    "        self.decoder = Decoder(config.num_decoder_layers, config.emb_dim, config.num_heads, config.ffn_dim, config.dropout)\n",
    "        self.linear = nn.Linear(config.emb_dim, config.vocab_size)\n",
    "    \n",
    "    def forward(self, original, target, target_mask=None, padding_mask=None):\n",
    "        original = self.embedding(original)\n",
    "        target = self.embedding(target)\n",
    "        encoder_out = self.encoder(original)\n",
    "        decoder_out = self.decoder(target, encoder_out, target_mask, padding_mask)\n",
    "        output = self.linear(decoder_out)\n",
    "        \n",
    "        # print(\"embdedding original\", \"-\" * 100, \"\\n\", original)\n",
    "        # print(\"embdedding target\", \"-\" * 100, \"\\n\", target)\n",
    "        # print(\"encoder_out\", \"-\" * 100, \"\\n\", encoder_out)\n",
    "        # print(\"decoder_out\", \"-\" * 100, \"\\n\", decoder_out) \n",
    "        # print(\"output\", \"-\" * 100, \"\\n\", output)\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    \n",
    "class Tranformer_config():\n",
    "    def __init__(self) -> None:\n",
    "        self.emb_dim = 150\n",
    "        self.max_len = 200\n",
    "        self.vocab_size = 50257\n",
    "        \n",
    "        self.num_encoder_layers = 6\n",
    "        self.num_decoder_layers = 6\n",
    "        self.num_heads = 2\n",
    "        \n",
    "        self.ffn_dim = 2048\n",
    "        self.dropout = 0.1\n",
    "        \n",
    "        self.batch_size = 1\n",
    "        self.lr = 1e-4\n",
    "        self.epochs = 10\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        \n",
    "config = Tranformer_config()\n",
    "transformer = Transformer(config)\n",
    "output = transformer(torch.randint(0, 50257, [1, 200]), torch.randint(0, 50257, [1, 200]))\n",
    "# print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
